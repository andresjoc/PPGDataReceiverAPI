<!--
PPGDataReceiverAPI - README
Generated by GitHub Copilot assistant.
-->

# 🚀 PPG Data Receiver API

A lightweight collector and viewer for Photoplethysmography (PPG) streams  built to receive PPG data from smartwatches and other sources, broadcast it via WebSocket, and persist incoming data as CSV files for later analysis.

> Friendly, simple, and file-backed: send JSON to the backend, watch data live in the frontend, and keep CSV archives in the `data/` folder.

---

## 📚 Table of contents

- [What is this project?](#what-is-this-project)
- [Key features](#key-features)
- [Architecture](#architecture)
- [Repository layout](#repository-layout)
- [Data format](#data-format)
- [API & WebSocket](#api-websocket)
- [Frontend](#frontend)
- [Storing & CSVs](#storing-csvs)
- [Quickstart](#quickstart)
- [Manual (by-hand) setup](#manual-by-hand-setup)

---

<a id="what-is-this-project"></a>
## 🤔 What is this project?

This repo provides a simple backend that accepts PPG data (JSON) over HTTP, broadcasts incoming samples to browser clients via WebSocket, and saves each received batch into a timestamped CSV file. It also includes a minimal frontend that can connect to the WebSocket and display live PPG samples in a chart.

Use cases:

- Collect PPG export data from wearables or small devices.
- Replay/visualize data in a browser during testing or demos.
- Keep a CSV log of every received payload for offline analysis.

---

<a id="key-features"></a>
## ✨ Key features

- HTTP POST receiver for PPG JSON payloads (`POST /`).
- WebSocket endpoint (`/ws`) that broadcasts incoming data to connected clients.
- CSV storage: each POSTed batch is persisted in `data/` with a UTC timestamp prefix.
- Simple frontend to connect via WebSocket and plot data (vanilla JS).
- **Dual-view visualization**: Frontend supports switching between "Original" (raw stream) and "Procesado" (processed 10s window) views via tabs.
- **Inference visualization**: Displays classification results (SR/AF) and confidence directly on the processed signal charts.
- Delta-format support: backend can accept either absolute samples or delta-encoded arrays.
 - Optional ML inference: the backend can load a TensorFlow/Keras model (see `backend/infer.py`) from the environment variable `PPG_MODEL_PATH` and run a classification on each received batch.

---

<a id="architecture"></a>
## 🏗️ Architecture

The backend is a small FastAPI app which:

- Accepts POST JSON payloads at `/` and converts them into a pandas DataFrame.
- Broadcasts the JSON/frames to WebSocket clients connected to `/ws`.
- Saves the DataFrame to the `data/` folder using a UTC timestamp prefix (`YYYY-MM-DDTHH-MM-SSZ_ppg.csv`).
 - Optionally loads a TensorFlow model from the path in `PPG_MODEL_PATH` and classifies incoming batches using the `Inferer` wrapper in `backend/infer.py`.

Simple diagram:

```
[ device / smartwatch ] --HTTP POST JSON--> [ FastAPI backend ] --save--> data/*.csv
                                                |
                                                +-- WebSocket (/ws) --push--> [ frontend (browser) ]
```

---

<a id="repository-layout"></a>
## 📁 Repository layout

Key files and folders:

| Path | Purpose |
|---|---|
| `backend/main.py` | FastAPI app: HTTP POST `/` and WebSocket `/ws`. Uses `data.py` helpers. |
| `backend/data.py` | Utilities converting PPG JSON to pandas DataFrame and saving CSVs. Handles delta encoding. |
| `frontend/index.html` | Minimal UI to connect and display PPG streams. |
| `frontend/index.js`, `frontend/ws-client.js` | Frontend client and chart setup. |
| `data/` | Where incoming CSVs are stored. Example files present. |
| `backend/infer.py` | Optional inference wrapper that loads a TensorFlow/Keras model and classifies PPG DataFrames. |
| `models/` | (gitignored) Optional model artifacts (e.g. `.keras`, `.h5`). Place trained models here for local testing. |
| `requirements.txt` | Python dependencies for backend. |
| `start.sh`, `start.bat` | Convenience scripts to launch the backend (shell / PowerShell). |

---

<a id="data-format"></a>
## 🧾 Data format

The backend expects a JSON object containing PPG arrays. The primary keys are:

- `TIMESTAMP`  array of sample timestamps (used as DataFrame index).
- `RED`, `IR`, `GREEN`  arrays with sensor channel values.

The backend also accepts delta-encoded variants. If the payload uses delta-encoding the keys are suffixed with `_DELTA` (for example `TIMESTAMP_DELTA`, `RED_DELTA`). The server will convert delta arrays into absolute values before broadcasting or saving.

Examples (simplified):

```json
{
  "TIMESTAMP": [163... , 163..., ...],
  "RED": [12345, 12348, ...],
  "IR": [54321, 54325, ...]
}
```

Or delta-format:

```json
{
  "TIMESTAMP_DELTA": [1000, 10, 10],
  "RED_DELTA": [12000, 3, 2]
}
```

The backend function `ppg_dict_to_dataframe` (see `backend/data.py`) handles conversion and delta decoding.

---

## 🔬 Signal processing & inference

The project includes an optional inference path implemented in `backend/infer.py`. When a model path is provided via the environment variable `PPG_MODEL_PATH` the backend instantiates an `Inferer` which buffers incoming samples and runs a classification once enough data is available. Key details:

- Input length & sampling:
  - The inference code expects exactly 10 seconds of data sampled at 25 Hz (250 samples). If incoming posts are shorter the `Inferer` maintains an internal rolling buffer until 250 samples are accumulated.
  - The implementation verifies sampling frequency by inspecting timestamp differences and will raise an error if the frequency deviates significantly from 25 Hz.

- Preprocessing steps (applied per channel):
  1. Band-pass filtering: a 4th-order Butterworth bandpass between 0.5 Hz and 8.0 Hz is applied to remove baseline wander and high-frequency noise. Filtering is applied with zero-phase filtering (`scipy.signal.filtfilt`) to avoid phase distortion.
  2. Robust normalization: each channel is centered by its median and scaled by the median absolute deviation (MAD) to reduce the influence of outliers and amplitude differences across sensors.

- Model input and output:
  - Models must accept input shaped like `(1, 250, 1)` (batch size 1, 250 time steps, 1 channel per prediction). The `infer` wrapper prepares each channel as a separate input and runs the model per-channel.
  - The model output is interpreted as a per-class probability vector. The inference code maps the highest-probability class index `0 -> 'SR'` (sinus rhythm) and `1 -> 'AF'` (atrial fibrillation) and records the maximum probability as `confidence`.

- Results returned:
  - For each channel the inference routine returns a small result dictionary containing:
    - `original_signal`: the raw 250-sample signal (float32)
    - `preprocessed_signal`: the filtered + normalized signal (float32)
    - `label`: `'SR'` or `'AF'`
    - `confidence`: float in `[0.0, 1.0]`

- Practical notes:
  - The `Inferer` class concatenates incoming batches and keeps the most recent 250 samples; it therefore works with streaming or batched POSTs as long as timestamps and sample rate are consistent.
  - Models are loaded with `keras.models.load_model(..., compile=False)` so a saved Keras model file (`.keras`, `.h5`) is expected.
  - Because TensorFlow and numeric packages are required, installing `tensorflow`, `numpy` and `scipy` is necessary when using inference (see `requirements.txt`).

Example (logged output printed by `backend/main.py` when a classification occurs):

```
Classification results:
  Channel RED: AF (confidence: 0.912)
  Channel GREEN: SR (confidence: 0.987)
```


---

<a id="api-websocket"></a>
## 🔌 API & WebSocket

- POST `/`  Accepts JSON body with PPG data. Returns `{"status": "ok", "received": true}` on success. The server will:
  - Convert JSON into a pandas DataFrame.
  - Broadcast a JSON payload to WebSocket clients containing:
    - `raw`: The original data batch (JSON orient=`split`).
    - `inference`: (Optional) Classification results, preprocessed signals, and confidence scores if a model is loaded and a full window is available.
  - Save the DataFrame to `data/<UTC-prefix>_ppg.csv`.

- WebSocket `/ws`  Connect with a browser or tool to receive live updates. The backend restricts connections to localhost for basic safety (only `127.0.0.1`, `::1`, or `localhost` are allowed).

Example curl to POST (replace `payload.json` with your data):

```bash
curl -X POST "http://localhost:8000/" -H "Content-Type: application/json" --data-binary @payload.json
```

Example WebSocket client (JS snippet):

```js
const ws = new WebSocket('ws://localhost:8000/ws');
ws.onmessage = (ev) => {
  const msg = ev.data;
  console.log('PPG update:', msg);
};

// To send from a device that speaks WebSocket (not required for HTTP-post flow):
ws.onopen = () => ws.send(JSON.stringify(yourPpgObject));
```

---



<a id="frontend"></a>
## 🖥️ Frontend

The `frontend/` folder contains a minimal UI built with plain JavaScript to connect to `/ws`, receive the broadcasted JSON frames, and render them into a chart. Files of interest:

- `frontend/index.html`  entry page with tab structure.
- `frontend/ws-client.js`  WebSocket client helper.
- `frontend/chart-setup.js`  charting logic for both raw and processed views.
- `frontend/data-handler.js`  parses the unified WebSocket payload.

The frontend supports two visualization modes, switchable via tabs:

- **Original View**: Real-time scrolling plots of the raw PPG signals.
- **Processed View**: Static plots of the last 10-second window, showing the preprocessed signal and the inference result (Label + Confidence) in the chart title.

You can adapt the frontend to your visualization stack (Chart.js, D3, etc.).

![Frontend preview](screenshots/frontend.png)

---

<a id="storing-csvs"></a>
## 💾 Storing & CSVs

Incoming POSTed payloads are converted into a pandas DataFrame and saved as CSV in the `data/` folder. Filenames use a UTC timestamp prefix, for example:

```
data/2025-11-17T01-25-09Z_ppg.csv
```

The CSVs can be loaded with pandas or any spreadsheet tool for offline analysis.
 
---

<a id="quickstart"></a>
## ⚡ Quickstart

Recommended: use the bundled `prepare.sh` script to create a virtual environment, install dependencies, and prepare the project, then start the app from the created environment. Take in consideration that Python with 3.9 <= version <= 3.12 is required.

Run on Unix-like systems (bash / WSL / Git Bash):

```bash
./prepare.sh
source .venv/bin/activate
./start.sh
```

The `prepare.sh` script is idempotent and prints next steps after setup.

<a id="manual-by-hand-setup"></a>
## 🛠️ Manual (by-hand) setup

If you prefer not to use `prepare.sh` you can set up and run the project manually.

1. Create and activate a virtual environment

```bash
# create venv
python -m venv .venv
# activate (bash / WSL / Git Bash)
source .venv/bin/activate
```

2. Install dependencies

```bash
python -m pip install --upgrade pip setuptools wheel
python -m pip install -r requirements.txt
```

3. Run the backend

```bash
# development with reload
python -m uvicorn backend.main:app --host 0.0.0.0 --port 8000 --reload
```

Or use the provided start script:

```bash
./start.sh        # bash
```

Environment variables recognized by `start.sh` include `PPG_DATA_DIR` (directory for received CSVs) and `PPG_MODEL_PATH` (path to a TensorFlow/Keras model file). `start.sh` will canonicalize and export these paths and prints diagnostic lines on startup.

4. Serve the frontend (optional)

```bash
cd frontend
python -m http.server 8080
# then open http://localhost:8080/
```

5. Send test data

```bash
curl -X POST "http://localhost:8000/" -H "Content-Type: application/json" --data-binary @payload.json
```

Notes:

- If you run the backend on a remote host, update the frontend WebSocket URL or allow the host in the backend CORS settings (`FRONTEND_PORT` environment variable influences allowed origins).
- The backend will save received batches to the `data/` folder as `YYYY-MM-DDTHH-MM-SSZ_ppg.csv`.
